{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Artifacts analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "d:\\workspace\\repos\\deepfix\\.venv\\Lib\\site-packages\\deepchecks\\core\\serialization\\dataframe\\html.py:16: UserWarning:\n",
            "\n",
            "pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from deepfix.core.agents.coordinators import ArtifactAnalysisCoordinator\n",
        "from deepfix.core.config import MLflowConfig,ArtifactConfig"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Ingest Image Classification Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "from deepfix.core.pipelines.factory import DatasetLoggingPipeline\n",
        "from deepfix.zoo.datasets.foodwaste import load_train_and_val_datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load image datasets\n",
        "train_data, val_data = load_train_and_val_datasets(\n",
        "    image_size=448,\n",
        "    batch_size=8,\n",
        "    num_workers=4,\n",
        "    pin_memory=False,)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dataset_name=\"cafetaria-foodwaste-lstroetmann\"\n",
        "\n",
        "dataset_logging_pipeline = DatasetLoggingPipeline(dataset_name=dataset_name,\n",
        "                                                train_test_validation=True,\n",
        "                                                data_integrity=True,\n",
        "                                                batch_size=8,\n",
        "                                                overwrite=False # True -> i.e. delete and re-create\n",
        "                                                )\n",
        "\n",
        "dataset_logging_pipeline.run(train_data=train_data,\n",
        "                            test_data=val_data,\n",
        "                        )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Analyze Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "mlflow_config = MLflowConfig()\n",
        "artifact_config = ArtifactConfig(dataset_name=\"cafetaria-foodwaste-lstroetmann\",\n",
        "                                load_dataset_metadata=True,\n",
        "                                load_checks=False,\n",
        "                                load_model_checkpoint=False,\n",
        "                                load_training=False,\n",
        "                                download_if_missing=True,\n",
        "                                cache_enabled=True\n",
        "                            )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "coordinator = ArtifactAnalysisCoordinator.from_config(mlflow_config=mlflow_config, \n",
        "                                            artifact_config=artifact_config,\n",
        "                                            env_file=\"D:/workspace/repos/deepfix/.env\")\n",
        "result = coordinator.run()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "DEEPFIX ANALYSIS RESULT\n",
            "================================================================================\n",
            "Dataset: cafetaria-foodwaste-lstroetmann\n",
            "Run ID: None\n",
            "================================================================================\n",
            "Summary of the analysis:\n",
            "This dataset exhibits multiple critical flaws that fundamentally undermine its suitability for computer vision modeling. The most severe issue is the completely non-representative train-test split, evidenced by both statistical analysis and deepchecks validation. \n",
            "\n",
            "**Core Problems:**\n",
            "1. **Catastrophic Label Distribution Mismatch**: 75% of test set labels are absent from the training set, with specific labels ('4', '1', '37') appearing exclusively in testing. This makes performance metrics completely unreliable as the model is being tested on classes it never encountered during training.\n",
            "\n",
            "2. **Severe Statistical Drift**: Multiple image properties show significant differences between sets - brightness (0.43 KS), contrast (0.5 KS), and color channel intensities (0.82-0.96 KS). This indicates the train and test data likely come from different sources or collection conditions.\n",
            "\n",
            "3. **Extremely Inadequate Dataset Size**: With only 215 training samples and 160 test samples, the dataset is orders of magnitude too small for meaningful computer vision model development, especially given the complexity of food waste detection tasks.\n",
            "\n",
            "4. **Distribution Inconsistencies**: Beyond the label issues, basic statistical properties (means, standard deviations) differ significantly between sets, confirming the split is neither random nor representative.\n",
            "\n",
            "**Critical Recommendations:**\n",
            "- **Immediate Priority**: Completely redo the train-test split using stratified sampling to ensure proper label distribution representation. Cross-validation should be employed given the small dataset size.\n",
            "- **Data Collection Imperative**: Significantly expand the dataset with diverse real-world examples. The current size is fundamentally insufficient for model generalization.\n",
            "- **Augmentation Strategy**: Implement comprehensive data augmentation (flips, rotations, color adjustments) to artificially increase diversity, though this cannot substitute for more real data.\n",
            "- **Quality Control**: Establish consistent data collection protocols to prevent the image property differences observed between sets.\n",
            "\n",
            "Without addressing these foundational data quality issues first, any modeling efforts will produce misleading results and fail to generalize to real-world scenarios.\n",
            "================================================================================================================================================================\n",
            "SUMMARY STATISTICS\n",
            "================================================================================\n",
            "Total findings: 6\n",
            "Agents involved: ['DatasetArtifactsAnalyzer', 'DeepchecksArtifactsAnalyzer']\n",
            "Severity distribution:\n",
            "{'high': 5, 'medium': 1}\n",
            "Priority distribution:\n",
            "{'high': 4, 'medium': 2}\n",
            "================================================================================\n",
            "HIGH SEVERITY ISSUES\n",
            "================================================================================\n",
            "1. [DatasetArtifactsAnalyzer] Extremely small dataset size for computer vision task\n",
            "   Evidence: Training set has only 215 samples, test set has 160 samples\n",
            "   Action: Implement comprehensive data augmentation strategies\n",
            "   Rationale: With only 215 training samples, the model will inevitably memorize the dataset rather than learn general features. Augmentation (flips, rotations, color adjustments, cropping) can artificially increase dataset diversity and size.\n",
            "2. [DatasetArtifactsAnalyzer] Insufficient data for robust model generalization\n",
            "   Evidence: Total dataset size of 375 samples is inadequate for most computer vision models to learn meaningful patterns without overfitting\n",
            "   Action: Prioritize collecting more diverse real-world data\n",
            "   Rationale: While augmentation helps, there is no substitute for more authentic data. Expanding the dataset is the most effective long-term solution for improving generalization and reducing overfitting in food waste detection tasks.\n",
            "3. [DeepchecksArtifactsAnalyzer] Severe label distribution mismatch between training and test sets\n",
            "   Evidence: 75% of labels found in test set were not in train set. New labels most common in test set: ['4', '1', '37']\n",
            "   Action: Re-evaluate and recreate the train-test split to ensure proper label distribution\n",
            "   Rationale: The current split contains fundamentally different label distributions, making validation metrics unreliable and preventing meaningful model evaluation\n",
            "4. [DeepchecksArtifactsAnalyzer] High statistical drift in image properties between train and test sets\n",
            "   Evidence: Found 5 numeric properties with Kolmogorov-Smirnov above threshold: {'Brightness': '0.43', 'RMS Contrast': '0.5', 'Mean Red Relative Intensity': '0.82', 'Mean Green Relative Intensity': '0.81', 'Mean Blue Relative Intensity': '0.96'}\n",
            "   Action: Investigate the source of image property differences and ensure consistent data collection methods\n",
            "   Rationale: Significant differences in image properties suggest the train and test data come from different sources or conditions, leading to unreliable performance metrics\n",
            "5. [DeepchecksArtifactsAnalyzer] Extreme label drift indicating non-representative test set\n",
            "   Evidence: Found 1 categorical label properties with Cramer's V above threshold: {'Samples Per Class': '0.92'}\n",
            "   Action: Use stratified sampling or cross-validation to ensure representative splits\n",
            "   Rationale: High label drift scores indicate the test set doesn't represent the training distribution, making it impossible to assess true model generalization\n",
            "================================================================================\n",
            "MEDIUM SEVERITY ISSUES\n",
            "================================================================================\n",
            "1. [DatasetArtifactsAnalyzer] Distribution shift between training and test sets\n",
            "   Evidence: Test set standard deviations (0.263, 0.263, 0.265) are significantly higher than training set (0.191, 0.207, 0.242), and test means differ notably in second and third channels\n",
            "   Action: Re-evaluate the train-test split methodology\n",
            "   Rationale: The distribution differences suggest the split may not be random or representative. Ensure both sets come from the same underlying distribution to get reliable performance metrics and prevent hidden overfitting.\n",
            "================================================================================\n",
            "AGENT-SPECIFIC ANALYSIS\n",
            "================================================================================\n",
            "DatasetArtifactsAnalyzer:\n",
            "  - Findings: 3\n",
            "  - Artifacts analyzed: DatasetArtifacts\n",
            "DeepchecksArtifactsAnalyzer:\n",
            "  - Findings: 3\n",
            "  - Artifacts analyzed: DeepchecksArtifacts\n"
          ]
        }
      ],
      "source": [
        "print(result.to_text())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "#print(result.summary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "#ctx = result.context"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "#print(ctx.to_summary())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
