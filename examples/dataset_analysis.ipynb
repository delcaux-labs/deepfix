{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "d:\\workspace\\repos\\deepfix\\.venv\\Lib\\site-packages\\deepchecks\\core\\serialization\\dataframe\\html.py:16: UserWarning:\n",
            "\n",
            "pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from deepfix.core.agents import DatasetAnalyzer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Image classification Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Ingestion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from deepfix.core.pipelines.factory import DatasetIngestionPipeline\n",
        "from deepfix.zoo.datasets.foodwaste import load_train_and_val_datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load image datasets\n",
        "train_data, val_data = load_train_and_val_datasets(\n",
        "    image_size=448,\n",
        "    batch_size=8,\n",
        "    num_workers=4,\n",
        "    pin_memory=False,)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dataset_name=\"cafetaria-foodwaste-lstroetmann\"\n",
        "\n",
        "dataset_logging_pipeline = DatasetIngestionPipeline(dataset_name=dataset_name,\n",
        "                                                train_test_validation=True,\n",
        "                                                data_integrity=True,\n",
        "                                                batch_size=8,\n",
        "                                                overwrite=False # True -> i.e. delete and re-create\n",
        "                                                )\n",
        "\n",
        "dataset_logging_pipeline.run(train_data=train_data,\n",
        "                            test_data=val_data,\n",
        "                        )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Analyze Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "env_file=\"D:/workspace/repos/deepfix/.env\"\n",
        "dataset_name=\"cafetaria-foodwaste-lstroetmann\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "analyzer = DatasetAnalyzer(env_file=env_file,)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "result = analyzer.run(dataset_name=dataset_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "DEEPFIX ANALYSIS RESULT\n",
            "\n",
            "Dataset: cafetaria-foodwaste-lstroetmann\n",
            "Run ID: None\n",
            "================================================================================\n",
            "Summary of the analysis:\n",
            "The cross-artifact analysis reveals a critically flawed dataset with multiple severe issues that would prevent successful model development. The most critical problem is the catastrophic label mismatch where 75% of test labels are completely unseen in training, making evaluation meaningless. Combined with an extremely small dataset size (215 train samples) and systematic distribution mismatches across color channels, brightness, and contrast, this dataset requires fundamental restructuring. The core recommendations are to completely recreate the train-test split using stratified sampling, aggressively collect more data, implement comprehensive augmentation, and audit the data pipeline to address systematic distribution issues. Without these interventions, any model training would be fundamentally compromised.\n",
            "================================================================================================================================================================\n",
            "SUMMARY STATISTICS\n",
            "================================================================================\n",
            "Total findings: 10\n",
            "Agents involved: ['DatasetArtifactsAnalyzer', 'DeepchecksArtifactsAnalyzer', 'CrossArtifactReasoningAgent']\n",
            "Severity distribution:\n",
            "{'high': 8, 'medium': 2}\n",
            "Priority distribution:\n",
            "{'high': 6, 'medium': 4}\n",
            "================================================================================\n",
            "HIGH SEVERITY ISSUES\n",
            "================================================================================\n",
            "1. [DatasetArtifactsAnalyzer] Extremely small dataset size\n",
            "   Evidence: Only 215 training samples and 160 test samples, which is insufficient for robust computer vision model training\n",
            "   Action: Implement aggressive data augmentation and prioritize collecting more real-world data\n",
            "   Rationale: Small datasets force models to memorize rather than generalize, leading to severe overfitting. Augmentation can artificially expand the dataset while more real data provides better coverage of the problem domain\n",
            "2. [DatasetArtifactsAnalyzer] Limited feature diversity due to small sample size\n",
            "   Evidence: With only 215 training samples, the dataset likely lacks sufficient variation in lighting conditions, angles, and food waste scenarios\n",
            "   Action: Implement comprehensive data augmentation including color jitter, rotations, flips, and brightness adjustments\n",
            "   Rationale: Augmentation increases feature diversity and prevents models from learning dataset-specific artifacts rather than general food waste patterns\n",
            "3. [DeepchecksArtifactsAnalyzer] Severe label mismatch between train and test sets\n",
            "   Evidence: 75% of labels found in test set were not present in train set, with new labels ['4', '1', '37'] being most common\n",
            "   Action: Re-evaluate and recreate the train-test split using stratified sampling to ensure all classes are represented in both sets\n",
            "   Rationale: Testing on unseen classes makes model evaluation meaningless and prevents accurate performance assessment\n",
            "4. [DeepchecksArtifactsAnalyzer] Extreme label distribution drift between datasets\n",
            "   Evidence: Cramer's V score of 0.92 for 'Samples Per Class' indicates severe categorical drift, far exceeding the 0.15 threshold\n",
            "   Action: Implement stratified k-fold cross-validation to ensure representative sampling across all splits\n",
            "   Rationale: High label drift indicates non-random splitting that biases model training and evaluation\n",
            "5. [DeepchecksArtifactsAnalyzer] Significant visual property drift across multiple image characteristics\n",
            "   Evidence: Kolmogorov-Smirnov scores above threshold for Brightness (0.43), Contrast (0.5), and color channels (Red: 0.82, Green: 0.81, Blue: 0.96)\n",
            "   Action: Apply data augmentation with color jitter, brightness adjustment, and contrast normalization to make model robust to visual variations\n",
            "   Rationale: Visual property drift indicates train and test sets have different imaging conditions, requiring augmentation for generalization\n",
            "6. [CrossArtifactReasoningAgent] Catastrophic label mismatch between train and test sets\n",
            "   Evidence: 75% of test labels were not present in training set, with new labels ['4', '1', '37'] appearing in test only. Combined with Cramer's V score of 0.92 indicating extreme categorical drift\n",
            "   Action: Completely recreate the dataset split using stratified sampling or collect additional data to ensure all classes are represented in both sets\n",
            "   Rationale: Testing on completely unseen classes makes model evaluation meaningless and prevents any meaningful performance assessment\n",
            "7. [CrossArtifactReasoningAgent] Extremely insufficient dataset size for robust training\n",
            "   Evidence: Only 215 training samples and 160 test samples total, which is orders of magnitude too small for effective computer vision model development\n",
            "   Action: Implement aggressive data collection strategy and comprehensive data augmentation including color jitter, rotations, flips, and brightness adjustments\n",
            "   Rationale: Small datasets force memorization rather than generalization. Augmentation can artificially expand diversity while more real data provides better domain coverage\n",
            "8. [CrossArtifactReasoningAgent] Systematic data distribution issues across multiple dimensions\n",
            "   Evidence: Significant mismatches in color channels (Blue: train mean=0.635 vs test mean=0.684), brightness (KS: 0.43), contrast (KS: 0.5), and severe visual property drift across all color channels\n",
            "   Action: Audit entire data collection and preprocessing pipeline to identify source of distribution discrepancies and implement normalization procedures\n",
            "   Rationale: Consistent distribution mismatches across multiple validation checks indicate fundamental methodology issues that must be addressed at the source\n",
            "================================================================================\n",
            "MEDIUM SEVERITY ISSUES\n",
            "================================================================================\n",
            "1. [DatasetArtifactsAnalyzer] Train-test distribution mismatch in color channels\n",
            "   Evidence: Blue channel shows significant differences: train mean=0.635 vs test mean=0.684, train std=0.242 vs test std=0.265, indicating potential data split issues\n",
            "   Action: Re-evaluate data splitting strategy and ensure proper randomization\n",
            "   Rationale: Distribution mismatches between train and test sets lead to unreliable performance evaluation and hidden overfitting, as the test set may not properly represent the training distribution\n",
            "2. [DeepchecksArtifactsAnalyzer] Potential data collection or splitting methodology issues\n",
            "   Evidence: Multiple validation failures across label distribution, new labels, and image properties suggest systematic problems with data handling\n",
            "   Action: Audit data collection and preprocessing pipeline to identify where the distribution discrepancies originate\n",
            "   Rationale: Consistent failures across multiple validation checks indicate fundamental issues with data acquisition or preparation methodology\n",
            "================================================================================\n",
            "AGENT-SPECIFIC ANALYSIS\n",
            "================================================================================\n",
            "DatasetArtifactsAnalyzer:\n",
            "  - Findings: 3\n",
            "  - Artifacts analyzed: DatasetArtifacts\n",
            "DeepchecksArtifactsAnalyzer:\n",
            "  - Findings: 4\n",
            "  - Artifacts analyzed: DeepchecksArtifacts\n",
            "CrossArtifactReasoningAgent:\n",
            "  - Findings: 3\n",
            "  - Artifacts analyzed: DatasetArtifacts, DeepchecksArtifacts\n",
            "  - Summary: The cross-artifact analysis reveals a critically flawed dataset with multiple severe issues that wou...\n"
          ]
        }
      ],
      "source": [
        "print(result.to_text())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
